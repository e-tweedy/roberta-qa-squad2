{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf6086e-b156-4218-8e91-05b30e0cebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers evaluate accelerate\n",
    "!pip install -qU torch torchvision torchaudio\n",
    "!pip install -qU huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d14fab5-6ea0-4889-8a1b-c90800261254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi,HfFolder\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DefaultDataCollator,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    \n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02da422a-4c06-4280-827e-6609a7bfcfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386b58123ac44f968d2ccb15e923821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73baba-c107-409a-8b0f-a9fefd81788a",
   "metadata": {},
   "source": [
    "## SQuAD v2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129471d-2e34-4a13-bc1b-5b2cfcdd8b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5a0eaa-ec28-403c-810c-c0de28f4c530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9896214657034893b1c954e99fbbddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe9a4300ea04ecabc26c2963a99ea5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba70c9cd4084e6abbc4d9add2b457cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a775c0c21f894095a9d98fbdf50a0c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6734490c9e2b4158a69d9ccd7036c65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d9a0df4304ca29db3ee99fe9a7bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309536be99c3428d8b4b19f4bf0d08c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset('squad_v2',use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a914-7907-4ce0-a0a9-515783254b32",
   "metadata": {},
   "source": [
    "### Tokenize and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8b76b7-e87d-440a-882f-d698424f8d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d73839ef514d07aaca5529b5d78bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56b42d064c84688b329513a3103d067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98871cb43e542a9a6dbb8330999a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e50eb411fa4b68a47f737809c81cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e422d3-1ac4-4768-ab5c-9c8807514c1e",
   "metadata": {},
   "source": [
    "We will first preprocess our training and validation dataset using a custom function preprocess_examples.\n",
    "\n",
    "We'll do the following:\n",
    "* For each sample, tokenize the questions and context.  All questions are fairly short, but the contexts can be quite long.\n",
    "    * We use truncation while tokenizing the contexts to keep the pieces short.\n",
    "    * This truncation is done according to the parameters return_overflowing_tokens, max_length, stride, and padding.  Each tokenized context is broken into token sequences of length at most max_length, and consecutive ensequences overlap by 128 tokens (in order to make sure the entire answer appears in at least one sequence).\n",
    "    * Then all sequences are padded at the end using the padding token to become sequences of length max_length.\n",
    "* We'll use several important pieces of data from the output of the tokenizer:\n",
    "    * The overflow_to_sample_mapping which, for each tokenized sequence, provides the index of the sample from where that sequence came.\n",
    "    * The offset_mapping which, for each token in each tokenized sequence, provides a pair (start,end) giving the character positions spanned by that token in the sample.\n",
    "    * The sequence_ids which, for each sequence, give a list containing entries 0 (for tokens in coming from question), 1 (for tokens coming from context piece), and None (for special tokens)\n",
    "* Record for each sequence the starting and ending token position of the provided answer in the context:\n",
    "    * If the answer is not in that sequence, record start_position = end_position = 0\n",
    "    * If the answer is in that sequence, then:\n",
    "        * retrieve the start and end positions of the context piece from sequence_ids\n",
    "        * step inwards from start and end positions until we locate the answer, and record those positions\n",
    "* During evaluation, it will be helpful to have two additional columns.  Retrieving them slows down the mapping process however, and we'll only include them for the evaluation set:\n",
    "    * a modified version of the offset_mapping pairs - where the entries are the actual offset_mapping pairs for context tokens and None otherwise; this information comes from sequence_ids\n",
    "    * a column containing the example_id the sequence came from\n",
    "* Drop the columns from the original training data, so that the resulting dataset has columns:\n",
    "    * 'input_ids', 'attention_mask', 'start_positions', 'stop_positions'\n",
    "    * in the case of the validation set, also the modified 'offset_mapping' and 'example_id'\n",
    "    \n",
    "This is accomplished via a custom function preprocess_examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c21a02-ae9b-4dfe-bac9-02fea8211e3c",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008db849-bca8-454f-a108-15ee2b92f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5000bd85c644cac8268550c82d9a17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib.utils import preprocess_examples\n",
    "\n",
    "train_dataset = squad['train'].map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad['train'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer':tokenizer,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f34b6-6ecf-42bd-bfb1-578665d30750",
   "metadata": {},
   "source": [
    "#### Validation data\n",
    "Set `is_test=True` to retrieve additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d912b8-f46f-4fcb-b02a-e0257c208017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f683e06031c4d149186b7cc9824196f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_dataset = squad['validation'].map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad['validation'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer':tokenizer,\n",
    "        'is_test':True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9dc91e-cc8d-4d92-b315-4c7f4b1d4e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08edf574-5c60-494a-8586-d054835fe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d5a60-3ace-4338-be1f-f8ca0abb00de",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c5fa1-6f06-44c3-adcc-30e179772dc5",
   "metadata": {},
   "source": [
    "### Model, optimizer, accelerator, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6c97e1-1673-4bf2-ad27-5743935738b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5940f2533fcf4e62a7fac895433121d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 3e-5)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs=3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "output_dir = 'roberta-finetuned-squad-v2-accelerate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f5998-caa7-459d-8be1-549b254ab2c4",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cb16f9-f7e8-4e40-a760-a21b958a8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9b5fc2-05be-43f3-a219-f835815815e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6ed97ab9d049d981ded582b732ef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7598eee44c36407a95a077e4bc443533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cee270844694d2896a4a41872b2dca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact': 77.7478312136781, 'f1': 80.8702752323304, 'total': 11873, 'HasAns_exact': 78.91363022941971, 'HasAns_f1': 85.16747264397094, 'HasAns_total': 5928, 'NoAns_exact': 76.58536585365853, 'NoAns_f1': 76.58536585365853, 'NoAns_total': 5945, 'best_exact': 77.7478312136781, 'best_exact_thresh': 0.0, 'best_f1': 80.87027523233047, 'best_f1_thresh': 0.0}\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24810b374f984191b94be5c3ab6fd523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dc978693364760bc469de1801092f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'exact': 79.81133664617198, 'f1': 82.94151794597373, 'total': 11873, 'HasAns_exact': 78.2051282051282, 'HasAns_f1': 84.4744673705377, 'HasAns_total': 5928, 'NoAns_exact': 81.41295206055509, 'NoAns_f1': 81.41295206055509, 'NoAns_total': 5945, 'best_exact': 79.81133664617198, 'best_exact_thresh': 0.0, 'best_f1': 82.94151794597376, 'best_f1_thresh': 0.0}\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d92f79baf49f18149fc6500947fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b21facd7c44316adafb84bf32ad69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'exact': 80.45986692495578, 'f1': 83.52543495807724, 'total': 11873, 'HasAns_exact': 78.69433198380567, 'HasAns_f1': 84.83425932139885, 'HasAns_total': 5928, 'NoAns_exact': 82.22035323801514, 'NoAns_f1': 82.22035323801514, 'NoAns_total': 5945, 'best_exact': 80.45986692495578, 'best_exact_thresh': 0.0, 'best_f1': 83.52543495807726, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print('Evaluation!')\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "        \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "    \n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, squad['validation']\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir,save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8134dfe4-c780-4be3-83dd-27f8c3844143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta-finetuned-squad-v2-accelerate-run2/tokenizer_config.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/special_tokens_map.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/vocab.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/merges.txt',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/added_tokens.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d177a66-9f61-4840-969a-bf6ac651065c",
   "metadata": {},
   "source": [
    "## Model inference via hugging face hub inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f6c042-3ff4-405c-9767-e7a910004277",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 'etweedy/roberta-base-squad-v2'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45171e7d-c5eb-4371-99a9-7ed1dc2c2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "repo_id = \"etweedy/roberta-base-squad-v2\"\n",
    "\n",
    "QA_pipeline = pipeline('question-answering', model=repo_id, tokenizer=repo_id, handle_impossible_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f76d4e-2282-4cad-be8a-b0993585bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9599111080169678,\n",
       " 'start': 64,\n",
       " 'end': 85,\n",
       " 'answer': 'James Alexander Dewar'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    'question': 'Who invented Twinkies?',\n",
    "    'context': 'Twinkies were invented on April 6, 1930, by Canadian-born baker James Alexander Dewar for the Continental Baking Company in Schiller Park, Illinois.'\n",
    "}\n",
    "response = QA_pipeline(**input)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
