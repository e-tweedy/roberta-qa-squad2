{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106fda69-9320-4a9b-9ccb-8eb8666ebd44",
   "metadata": {},
   "source": [
    "# Fine-tuning RoBERTa base on SQuAD v2\n",
    "\n",
    "This Jupyter notebook demonstrates the process of fine-tuning a RoBERTa (Robustly optimized Bidirectional Encoder Representations from Transformers) model using pytorch and the ðŸ¤— libraries.\n",
    "\n",
    "* [Click here to visit the ðŸ¤— model card](https://huggingface.co/etweedy/roberta-base-squad-v2) to see more details about this model or [click here to visit a demonstration app for this model](https://huggingface.co/spaces/etweedy/roberta-squad-v2) hosted as a ðŸ¤— space.\n",
    "* The model is a fine-tuned version of [roberta-base for QA](https://huggingface.co/roberta-base)\n",
    "* It was fine-tuned for context-based extractive question answering on the [SQuAD v2 dataset](https://huggingface.co/datasets/squad_v2), a dataset of English-language context-question-answer triples designed for extractive question answering training and benchmarking.\n",
    "* Version 2 of SQuAD (Stanford Question Answering Dataset) contains the 100,000 examples from SQuAD Version 1.1, along with 50,000 additional \"unanswerable\" questions, i.e. questions whose answer cannot be found in the provided context.\n",
    "* The original RoBERTa (Robustly Optimized BERT Pretraining Approach) model was introduced in [this paper](https://arxiv.org/abs/1907.11692) and [this repository](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad47df-2d58-445f-a3f3-ee4fa5097f6e",
   "metadata": {},
   "source": [
    "Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2bea1-810e-47e8-aed0-426535fea76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers evaluate accelerate\n",
    "!pip install -qU torch torchvision torchaudio\n",
    "!pip install -qU huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d14fab5-6ea0-4889-8a1b-c90800261254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi,HfFolder\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DefaultDataCollator,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    \n",
    ")\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838b891-8599-46fb-b8fb-d5dfbf5b441b",
   "metadata": {},
   "source": [
    "Hugging Face API Token notebook login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02da422a-4c06-4280-827e-6609a7bfcfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5fb4731f3c4da49c6b462f011c4075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73baba-c107-409a-8b0f-a9fefd81788a",
   "metadata": {},
   "source": [
    "## SQuAD v2 dataset\n",
    "\n",
    "First we'll download the dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129471d-2e34-4a13-bc1b-5b2cfcdd8b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5a0eaa-ec28-403c-810c-c0de28f4c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89649876d64d444c9cad7b4806de2f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad = load_dataset('squad_v2',use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a914-7907-4ce0-a0a9-515783254b32",
   "metadata": {},
   "source": [
    "### Tokenize and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8b76b7-e87d-440a-882f-d698424f8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e422d3-1ac4-4768-ab5c-9c8807514c1e",
   "metadata": {},
   "source": [
    "We will preprocess our training and validation dataset using a custom function preprocess_examples.\n",
    "\n",
    "We'll do the following:\n",
    "* For each sample, tokenize the questions and context.  All questions are fairly short, but the contexts can be quite long.\n",
    "    * We use truncation while tokenizing the contexts to keep the pieces short.\n",
    "    * This truncation is done according to the parameters return_overflowing_tokens, max_length, stride, and padding.  Each tokenized context is broken into token sequences of length at most max_length, and consecutive ensequences overlap by 128 tokens (in order to make sure the entire answer appears in at least one sequence).\n",
    "    * Then all sequences are padded at the end using the padding token to become sequences of length max_length.\n",
    "* We'll use several important pieces of data from the output of the tokenizer:\n",
    "    * The overflow_to_sample_mapping which, for each tokenized sequence, provides the index of the sample from where that sequence came.\n",
    "    * The offset_mapping which, for each token in each tokenized sequence, provides a pair (start,end) giving the character positions spanned by that token in the sample.\n",
    "    * The sequence_ids which, for each sequence, give a list containing entries 0 (for tokens in coming from question), 1 (for tokens coming from context piece), and None (for special tokens)\n",
    "* Record for each sequence the starting and ending token position of the provided answer in the context:\n",
    "    * If the answer is not in that sequence, record start_position = end_position = 0\n",
    "    * If the answer is in that sequence, then:\n",
    "        * retrieve the start and end positions of the context piece from sequence_ids\n",
    "        * step inwards from start and end positions until we locate the answer, and record those positions\n",
    "* During evaluation, it will be helpful to have two additional columns.  Retrieving them slows down the mapping process however, and we'll only include them for the evaluation set:\n",
    "    * a modified version of the offset_mapping pairs - where the entries are the actual offset_mapping pairs for context tokens and None otherwise; this information comes from sequence_ids\n",
    "    * a column containing the example_id the sequence came from\n",
    "* Drop the columns from the original training data, so that the resulting dataset has columns:\n",
    "    * 'input_ids', 'attention_mask', 'start_positions', 'stop_positions'\n",
    "    * in the case of the validation set, also the modified 'offset_mapping' and 'example_id'\n",
    "    \n",
    "This is accomplished via a custom function preprocess_examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c21a02-ae9b-4dfe-bac9-02fea8211e3c",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008db849-bca8-454f-a108-15ee2b92f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-9cf05c80fcf103ec.arrow\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import preprocess_examples\n",
    "\n",
    "train_dataset = squad['train'].map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad['train'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer':tokenizer,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f34b6-6ecf-42bd-bfb1-578665d30750",
   "metadata": {},
   "source": [
    "#### Validation data\n",
    "Set `is_test=True` to retrieve additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d912b8-f46f-4fcb-b02a-e0257c208017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773096d6a5cb447aad4f68a3d9ac1e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_dataset = squad['validation'].map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    remove_columns=squad['validation'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer':tokenizer,\n",
    "        'is_test':True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9dc91e-cc8d-4d92-b315-4c7f4b1d4e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize dataloaders\n",
    "\n",
    "We prepare our dataloaders.  This training is best run on a CUDA device, i.e. a GPU.  If you're running into 'CUDA out of memory' errors, try lowering the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08edf574-5c60-494a-8586-d054835fe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d5a60-3ace-4338-be1f-f8ca0abb00de",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We first get set up for training:\n",
    "* We will use ðŸ¤— accelerators library to handle training on our GPU.  accelerators also automatically handles distributed training, if you're on a machine with multiple devices.\n",
    "* We'll use the AdamW (Adaptive moment estimation with Weight decay) optimization algorithm, with a linear learning rate scheduler.\n",
    "* We'll train for 3 epochs with a base learning rate of 3e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c5fa1-6f06-44c3-adcc-30e179772dc5",
   "metadata": {},
   "source": [
    "### Model, optimizer, accelerator, and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6c97e1-1673-4bf2-ad27-5743935738b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 3e-5)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs=3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "output_dir = 'roberta-finetuned-squad-v2-accelerate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f5998-caa7-459d-8be1-549b254ab2c4",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, we'll execute our training loop.  Notice that we don't need to include methods moving the model or batches to our CUDA device - accelerators handles this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cb16f9-f7e8-4e40-a760-a21b958a8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9b5fc2-05be-43f3-a219-f835815815e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6ed97ab9d049d981ded582b732ef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7598eee44c36407a95a077e4bc443533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cee270844694d2896a4a41872b2dca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact': 77.7478312136781, 'f1': 80.8702752323304, 'total': 11873, 'HasAns_exact': 78.91363022941971, 'HasAns_f1': 85.16747264397094, 'HasAns_total': 5928, 'NoAns_exact': 76.58536585365853, 'NoAns_f1': 76.58536585365853, 'NoAns_total': 5945, 'best_exact': 77.7478312136781, 'best_exact_thresh': 0.0, 'best_f1': 80.87027523233047, 'best_f1_thresh': 0.0}\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24810b374f984191b94be5c3ab6fd523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dc978693364760bc469de1801092f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'exact': 79.81133664617198, 'f1': 82.94151794597373, 'total': 11873, 'HasAns_exact': 78.2051282051282, 'HasAns_f1': 84.4744673705377, 'HasAns_total': 5928, 'NoAns_exact': 81.41295206055509, 'NoAns_f1': 81.41295206055509, 'NoAns_total': 5945, 'best_exact': 79.81133664617198, 'best_exact_thresh': 0.0, 'best_f1': 82.94151794597376, 'best_f1_thresh': 0.0}\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d92f79baf49f18149fc6500947fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b21facd7c44316adafb84bf32ad69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'exact': 80.45986692495578, 'f1': 83.52543495807724, 'total': 11873, 'HasAns_exact': 78.69433198380567, 'HasAns_f1': 84.83425932139885, 'HasAns_total': 5928, 'NoAns_exact': 82.22035323801514, 'NoAns_f1': 82.22035323801514, 'NoAns_total': 5945, 'best_exact': 80.45986692495578, 'best_exact_thresh': 0.0, 'best_f1': 83.52543495807726, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Eval step    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print('Evaluation!')\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)    \n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "    \n",
    "    # Concatenate logit arrays from batches    \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "    \n",
    "    # Compute and report metrics\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, squad['validation']\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir,save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8134dfe4-c780-4be3-83dd-27f8c3844143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta-finetuned-squad-v2-accelerate-run2/tokenizer_config.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/special_tokens_map.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/vocab.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/merges.txt',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/added_tokens.json',\n",
       " 'roberta-finetuned-squad-v2-accelerate-run2/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39054dbd-b734-4f1d-87e4-2bc8269e8fe7",
   "metadata": {},
   "source": [
    "##### Run 1:\n",
    "* 3 epochs\n",
    "* base_lr = 3e-5\n",
    "* linear scheduler\n",
    "* warmup = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103de2ba-56e9-43fb-89e5-b91894ad0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"exact\": 79.87029394424324,\n",
    "\"f1\": 82.91251169582613,\n",
    "\"total\": 11873,\n",
    "\"HasAns_exact\": 77.93522267206478,\n",
    "\"HasAns_f1\": 84.02838248389763,\n",
    "\"HasAns_total\": 5928,\n",
    "\"NoAns_exact\": 81.79983179142137,\n",
    "\"NoAns_f1\": 81.79983179142137,\n",
    "\"NoAns_total\": 5945}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81c8f1-c4d4-447d-9d9e-6af474212d1c",
   "metadata": {},
   "source": [
    "##### Run 2:\n",
    "\n",
    "same as Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bbdb4-3e21-4c52-9ab7-45e572f83058",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'exact': 80.45986692495578,\n",
    " 'f1': 83.52543495807724,\n",
    " 'total': 11873,\n",
    " 'HasAns_exact': 78.69433198380567,\n",
    " 'HasAns_f1': 84.83425932139885,\n",
    " 'HasAns_total': 5928,\n",
    " 'NoAns_exact': 82.22035323801514,\n",
    " 'NoAns_f1': 82.22035323801514,\n",
    " 'NoAns_total': 5945}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d177a66-9f61-4840-969a-bf6ac651065c",
   "metadata": {},
   "source": [
    "## Model inference via hugging face hub inference endpoint\n",
    "\n",
    "Inference using our saved model doesn't require much code - but don't forget to set the handle_impossible_answers option so that the pipeline properly handles the questions with impossible answers correctly - it will output '' for such a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f6c042-3ff4-405c-9767-e7a910004277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5057ad2edac48e9b491864b49dd6f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b98bbd3feac4cfc9f8c4c0be52674b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43586f5c59d4a95a4b0598d4dc46929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/351 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62863c1da0154306be29d843ac720ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9e261bcc0e490eaa1a0ec97255704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268eff8672c147caa660b1cb157ddae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8987f1b2154880882947b8a5d5ea16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repo_id = 'etweedy/roberta-base-squad-v2'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45171e7d-c5eb-4371-99a9-7ed1dc2c2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "repo_id = \"etweedy/roberta-base-squad-v2\"\n",
    "\n",
    "QA_pipeline = pipeline('question-answering', model=repo_id, tokenizer=repo_id, handle_impossible_answer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f76d4e-2282-4cad-be8a-b0993585bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9599111080169678,\n",
       " 'start': 64,\n",
       " 'end': 85,\n",
       " 'answer': 'James Alexander Dewar'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    'question': 'Who invented Twinkies?',\n",
    "    'context': 'Twinkies were invented on April 6, 1930, by Canadian-born baker James Alexander Dewar for the Continental Baking Company in Schiller Park, Illinois.'\n",
    "}\n",
    "response = QA_pipeline(**input)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bde92ac-5b89-474d-9680-319974fe3b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9994915127754211, 'start': 0, 'end': 0, 'answer': ''}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    'question': 'When was James Alexander Dewar born?',\n",
    "    'context': 'Twinkies were invented on April 6, 1930, by Canadian-born baker James Alexander Dewar for the Continental Baking Company in Schiller Park, Illinois.'\n",
    "}\n",
    "response = QA_pipeline(**input)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
